{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch import nn, optim\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Load the dataset\n",
    "# Assuming the dataset is in a CSV file named 'ecg_data.csv'\n",
    "# df = pd.read_csv('audit_risk.csv')\n",
    "# trial = pd.read_csv('trial.csv')\n",
    "\n",
    "# X = df.iloc[:, :-1]\n",
    "# y = df.iloc[:, -1]\n",
    "\n",
    "# X_test = trial.iloc[:, :-1]\n",
    "# y_test = trial.iloc[:, -1]\n",
    "\n",
    "class CNNClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNClassifier, self).__init__()\n",
    "        \n",
    "        self.CNN = nn.Conv2d(10, 32, kernel_size = 3, stride=1, padding = 1)\n",
    "        self.CNN2 = nn.Conv2d(32, 64, kernel_size = 3, stride=1, padding=1)\n",
    "        \n",
    "      \n",
    "        self.fc1 = nn.Linear(2048, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.fc3 = nn.Linear(512, 64)\n",
    "        self.fc4 = nn.Linear(64, 10)\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "           \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch, seq_len, input_size)\n",
    "        x = x.unsqueeze(1)\n",
    "        \n",
    "        # x shape: (batch, 1, seq_len, input_size)\n",
    "        x = self.CNN(x)\n",
    "        \n",
    "        x = F.relu(x)\n",
    "        x = self.CNN2(x)\n",
    "        \n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = x.reshape(1, -1)\n",
    "        \n",
    "        # Flatten the output\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc4(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        \n",
    "        x = x.reshape(10, 1)\n",
    "        \n",
    "        # Final output\n",
    "        \n",
    "       \n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Custom Dataset for Numerical Arrays\n",
    "class NumericalDataset(Dataset):\n",
    "    def __init__(self, X, y=None):\n",
    "        X = X.values\n",
    "        y = y.values\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32) if y is not None else None\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.y is not None:\n",
    "            return self.X[idx], self.y[idx]\n",
    "        return self.X[idx]\n",
    "    \n",
    "    def return_X_y(self):\n",
    "        return self.X, self.y\n",
    "\n",
    "# X = X.apply(pd.to_numeric, errors='coerce')\n",
    "# y = y.apply(pd.to_numeric, errors='coerce')\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "NumericalData = NumericalDataset(X_train, y_train)\n",
    "NumericalDataset_test = NumericalDataset(X_test, y_test)\n",
    "X_train, y_train = NumericalData.return_X_y()\n",
    "X_test, y_test = NumericalDataset_test.return_X_y()\n",
    "\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, num_features, dim=32, depth=4, heads=8, num_classes=1):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
    "        self.feature_embed = nn.Linear(num_features, dim)\n",
    "        \n",
    "        # Transformer Encoder\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=dim,\n",
    "                nhead=heads,\n",
    "                dim_feedforward=dim*4,\n",
    "                dropout=0.1,\n",
    "                activation='gelu',\n",
    "                batch_first=True\n",
    "            ),\n",
    "            num_layers=depth\n",
    "        )\n",
    "        \n",
    "        # Output Head\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, dim//2),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(dim//2, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch, num_features)\n",
    "        x = self.feature_embed(x)  # (batch, num_features, dim)\n",
    "        x = x.unsqueeze(1)  # (batch, 1, num_features, dim)\n",
    "        \n",
    "        x = x.unsqueeze(2)\n",
    "        # Add CLS token\n",
    "        cls_tokens = self.cls_token.expand(x.size(0), -1, -1)\n",
    "        \n",
    "        x = torch.mean(x, dim=2)  # (batch, num_features, dim)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)  # (batch, num_features+1, dim)\n",
    "        \n",
    "        # Transformer processing\n",
    "        x = self.transformer(x)\n",
    "        \n",
    "        # Use CLS token for prediction\n",
    "        cls_output = x[:, 0]\n",
    "        \n",
    "          # (batch, 1, dim)\n",
    "        \n",
    "          # Sigmoid for binary classification\n",
    "        x = self.mlp(cls_output)\n",
    "        x = torch.sigmoid(x)  # (batch, num_classes)\n",
    "        return cls_output, x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 0.7000\n",
      "tensor(nan)\n",
      "Epoch [1/5], Loss: 0.6967\n",
      "tensor(nan)\n",
      "Epoch [1/5], Loss: 0.6991\n",
      "tensor(nan)\n",
      "Epoch [1/5], Loss: 0.6892\n",
      "tensor(nan)\n",
      "Epoch [1/5], Loss: 0.6876\n",
      "tensor(nan)\n",
      "Epoch [1/5], Loss: 0.6841\n",
      "tensor(nan)\n",
      "Epoch [1/5], Loss: 0.7095\n",
      "tensor(nan)\n",
      "Epoch [1/5], Loss: 0.7126\n",
      "tensor(nan)\n",
      "Epoch [1/5], Loss: 0.7073\n",
      "tensor(nan)\n",
      "Epoch [1/5], Loss: 0.7015\n",
      "tensor(nan)\n",
      "Epoch [1/5], Loss: 0.6982\n",
      "tensor(nan)\n",
      "Epoch [1/5], Loss: 0.6902\n",
      "tensor(nan)\n",
      "Epoch [1/5], Loss: 0.6951\n",
      "tensor(nan)\n",
      "Epoch [1/5], Loss: 0.6963\n",
      "tensor(nan)\n",
      "Epoch [1/5], Loss: 0.6957\n",
      "tensor(nan)\n",
      "Epoch [1/5], Loss: 0.6952\n",
      "tensor(nan)\n",
      "Epoch [1/5], Loss: 0.6934\n",
      "tensor(nan)\n",
      "Epoch [1/5], Loss: 0.6934\n",
      "tensor(nan)\n",
      "Epoch [1/5], Loss: 0.6928\n",
      "tensor(nan)\n",
      "Epoch [1/5], Loss: 0.6926\n",
      "tensor(nan)\n",
      "Epoch [1/5], Loss: 0.6930\n",
      "tensor(nan)\n",
      "Epoch [1/5], Loss: 0.6966\n",
      "tensor(nan)\n",
      "Epoch [1/5], Loss: 0.6957\n",
      "tensor(nan)\n",
      "Epoch [1/5], Loss: 0.6932\n",
      "tensor(nan)\n",
      "Epoch [1/5], Loss: 0.6962\n",
      "tensor(nan)\n",
      "Epoch [1/5], Loss: 0.6953\n",
      "tensor(nan)\n",
      "Epoch [1/5], Loss: 0.6932\n",
      "tensor(nan)\n",
      "Epoch [1/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [1/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [1/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [1/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [1/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [1/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [1/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [1/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [1/5], Loss: 0.6932\n",
      "tensor(nan)\n",
      "Epoch [1/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [1/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [1/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [1/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [1/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [1/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [1/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [1/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [1/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [1/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [1/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [1/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [1/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [1/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [1/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [1/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [1/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [1/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [1/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [1/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [1/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [1/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [1/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [1/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [1/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [2/5], Loss: 0.6932\n",
      "tensor(nan)\n",
      "Epoch [2/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [2/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [2/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [2/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [2/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [2/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [2/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [2/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [2/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [2/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [2/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [2/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [2/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [2/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [2/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [2/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [2/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [2/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [2/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [2/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [2/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [2/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [2/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [2/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [2/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [2/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [2/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [2/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [2/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [2/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [2/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [2/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [2/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [2/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [2/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [2/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [2/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [2/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [2/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [2/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [2/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [2/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [2/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [2/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [2/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [2/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [2/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [2/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [2/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [2/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [2/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [2/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [2/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [2/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [2/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [2/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [2/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [2/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [2/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [2/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [3/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [3/5], Loss: 0.6932\n",
      "tensor(nan)\n",
      "Epoch [3/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [3/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [3/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [3/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [3/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [3/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [3/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [3/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [3/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [3/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [3/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [3/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [3/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [3/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [3/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [3/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [3/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [3/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [3/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [3/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [3/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [3/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [3/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [3/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [3/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [3/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [3/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [3/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [3/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [3/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [3/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [3/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [3/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [3/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [3/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [3/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [3/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [3/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [3/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [3/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [3/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [3/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [3/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [3/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [3/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [3/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [3/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [3/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [3/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [3/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [3/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [3/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [3/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [3/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [3/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [3/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [3/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [3/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [3/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [4/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [4/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [4/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [4/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [4/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [4/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [4/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [4/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [4/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [4/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [4/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [4/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [4/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [4/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [4/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [4/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [4/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [4/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [4/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [4/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [4/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [4/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [4/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [4/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [4/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [4/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [4/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [4/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [4/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [4/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [4/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [4/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [4/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [4/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [4/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [4/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [4/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [4/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [4/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [4/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [4/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [4/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [4/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [4/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [4/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [4/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [4/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [4/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [4/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [4/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [4/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [4/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [4/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [4/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [4/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [4/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [4/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [4/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [4/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [4/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [4/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [5/5], Loss: 0.6944\n",
      "tensor(nan)\n",
      "Epoch [5/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [5/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [5/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [5/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [5/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [5/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [5/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [5/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [5/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [5/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [5/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [5/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [5/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [5/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [5/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [5/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [5/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [5/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [5/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [5/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [5/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [5/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [5/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [5/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [5/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [5/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [5/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [5/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [5/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [5/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [5/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [5/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [5/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [5/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [5/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [5/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [5/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [5/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [5/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [5/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [5/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [5/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [5/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [5/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [5/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [5/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [5/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [5/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [5/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [5/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [5/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [5/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [5/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [5/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [5/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [5/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [5/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [5/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [5/5], Loss: 0.6931\n",
      "tensor(nan)\n",
      "Epoch [5/5], Loss: 0.6931\n",
      "tensor(nan)\n"
     ]
    }
   ],
   "source": [
    "MIN_DELTA = 0.0001\n",
    "MIN_LR = 1e-5\n",
    "class LRScheduler():\n",
    "    \"\"\"\n",
    "    Learning rate scheduler. If the validation loss does not decrease for the \n",
    "    given number of `patience` epochs, then the learning rate will decrease by\n",
    "    by given `factor`.\n",
    "    \"\"\"\n",
    "    def __init__(self, optimizer, patience=3, min_lr=MIN_LR, factor=0.1):\n",
    "        \"\"\"\n",
    "        new_lr = old_lr * factor\n",
    "        :param optimizer: the optimizer we are using\n",
    "        :param patience: how many epochs to wait before updating the lr\n",
    "        :param min_lr: least lr value to reduce to while updating\n",
    "        :param factor: factor by which the lr should be updated\n",
    "        \"\"\"\n",
    "        self.optimizer = optimizer\n",
    "        self.patience = patience\n",
    "        self.min_lr = min_lr\n",
    "        self.factor = factor\n",
    "        self.lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau( \n",
    "                self.optimizer,\n",
    "                mode='min',\n",
    "                patience=self.patience,\n",
    "                factor=self.factor,\n",
    "                min_lr=self.min_lr\n",
    "            )\n",
    "    def __call__(self, val_loss):\n",
    "        # take one step of the learning rate scheduler while providing the validation loss as the argument\n",
    "        self.lr_scheduler.step(val_loss)\n",
    "    \n",
    "model = Transformer(num_features=X.shape[-1], dim=32, depth=4, heads=8, num_classes=1)\n",
    "model2 = CNNClassifier()\n",
    "\n",
    "# model = model.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model2 = model2.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "criterion = nn.BCELoss()\n",
    "criterion2 = nn.BCELoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "optimizer2 = optim.Adam(model2.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "\n",
    "def scheduler(optimizer, step_size=10, gamma=0.1):\n",
    "    \"\"\"Step learning rate scheduler.\"\"\"\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "    return scheduler\n",
    "\n",
    "# Split data into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# # Convert to PyTorch tensors\n",
    "# X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# y_train_tensor = torch.tensor(y_train, dtype=torch.float32).to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# y_test_tensor = torch.tensor(y_test, dtype=torch.float32).to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# Training loop\n",
    "num_epochs = 5\n",
    "batch_size = 10\n",
    "train_loss_lst = []\n",
    "test_loss_lst = []\n",
    "# lr_schedule = LRScheduler(optimizer)\n",
    "num_batches = len(X) // batch_size\n",
    "for epoch in range(num_epochs):\n",
    "    model2.train()\n",
    "    \n",
    "    inputs = X_train\n",
    "    labels = y_train\n",
    "    for i in range(0, len(inputs), batch_size):    \n",
    "        # optimizer.zero_grad()\n",
    "        # outputs= model.forward(inputs)\n",
    "    \n",
    "        try: \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            \n",
    "            outputs, y_pred = model.forward(inputs[i:i+batch_size, :].to('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "            \n",
    "            label = labels[i:i+batch_size].to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "            label = label.unsqueeze(1)\n",
    "            \n",
    "            \n",
    "            \n",
    "        \n",
    "            loss = criterion(y_pred, label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer2.zero_grad()\n",
    "            \n",
    "            outputs2 = outputs.clone().detach()\n",
    "            \n",
    "            y_pred2 = model2.forward(outputs2)\n",
    "            \n",
    "            \n",
    "            loss2 = criterion2(y_pred2, label)\n",
    "            loss2.backward()\n",
    "            optimizer2.step()\n",
    "                \n",
    "            # print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss2.item():.4f}\")\n",
    "            train_loss_lst.append(loss2.item())\n",
    "        except:\n",
    "            continue\n",
    "    # Evaluation\n",
    "        model2.eval()\n",
    "        with torch.no_grad():\n",
    "            loss = []\n",
    "            preds = {}\n",
    "            # test_outputs, x = model(X_test_tensor)\n",
    "            for i in range(0, len(X_test), batch_size):\n",
    "                try: \n",
    "            # test_outputs = test_outputs.view(-1, 32, 1)\n",
    "                    \n",
    "                    test_outputs2, y_pred2 = model.forward(X_test[i:i+batch_size, :].to('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "                    \n",
    "                   \n",
    "                    test_loss = criterion(y_pred2, y_test[i:i+batch_size].unsqueeze(1))\n",
    "                    test_outputs2 = torch.where(test_outputs2.isnan(), fill, test_outputs2) \n",
    "                    pred = model2.forward(test_outputs2)\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    loss_2 = criterion2(pred, y_test[i:i+batch_size].to('cuda' if torch.cuda.is_available() else 'cpu').unsqueeze(1))\n",
    "                    loss.append(loss_2.item())\n",
    "                    for j in range(55, 80):\n",
    "                        if j ==0:\n",
    "                            preds[j] = []\n",
    "                        pre = i*0.01\n",
    "                        pre = torch.tensor(pre, dtype=torch.float32).to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "                        pred = torch.where(pred>pre, 1, 0)\n",
    "                    \n",
    "                        mask = list(pred==y_test[i:i+batch_size])\n",
    "                        preds[i].extend(mask)\n",
    "                    \n",
    "                except:\n",
    "                    continue\n",
    "            loss = torch.tensor(loss)\n",
    "            \n",
    "            print(torch.mean(loss))\n",
    "            for key, value in preds.items():\n",
    "                total = []\n",
    "                for v in value:\n",
    "                    for tr in v: \n",
    "                        if tr == True:\n",
    "                            total.append(1)\n",
    "                        else:\n",
    "                            total.append(0)\n",
    "                try: \n",
    "                    print(f\"Accuracy for batch {key}: {sum(total)/len(total):.4f}\")\n",
    "                except:\n",
    "                    continue\n",
    "        # Sa    ve the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Money_Value\n",
      "     Sector_score LOCATION_ID  PARA_A  Score_A  Risk_A  PARA_B  Score_B  \\\n",
      "642         55.57           4    0.23      0.2   0.046     0.0      0.2   \n",
      "\n",
      "     Risk_B  TOTAL  numbers  ...  RiSk_E  History  Prob  Risk_F  Score  \\\n",
      "642     0.0   0.23      5.0  ...     0.4        0   0.2     0.0    2.0   \n",
      "\n",
      "     Inherent_Risk  CONTROL_RISK  Detection_Risk  Audit_Risk  Risk  \n",
      "642          1.446           0.4             0.5      0.2892     0  \n",
      "\n",
      "[1 rows x 27 columns]\n",
      "Index([642], dtype='int64')\n",
      "Money_Value\n",
      "     Sector_score LOCATION_ID  PARA_A  Score_A  Risk_A  PARA_B  Score_B  \\\n",
      "0            3.89          23    4.18      0.6   2.508    2.50      0.2   \n",
      "1            3.89           6    0.00      0.2   0.000    4.83      0.2   \n",
      "2            3.89           6    0.51      0.2   0.102    0.23      0.2   \n",
      "3            3.89           6    0.00      0.2   0.000   10.80      0.6   \n",
      "4            3.89           6    0.00      0.2   0.000    0.08      0.2   \n",
      "..            ...         ...     ...      ...     ...     ...      ...   \n",
      "771         55.57           9    0.49      0.2   0.098    0.40      0.2   \n",
      "772         55.57          16    0.47      0.2   0.094    0.37      0.2   \n",
      "773         55.57          14    0.24      0.2   0.048    0.04      0.2   \n",
      "774         55.57          18    0.20      0.2   0.040    0.00      0.2   \n",
      "775         55.57          15    0.00      0.2   0.000    0.00      0.2   \n",
      "\n",
      "     Risk_B  TOTAL  numbers  ...  RiSk_E  History  Prob  Risk_F  Score  \\\n",
      "0     0.500   6.68      5.0  ...     0.4        0   0.2     0.0    2.4   \n",
      "1     0.966   4.83      5.0  ...     0.4        0   0.2     0.0    2.0   \n",
      "2     0.046   0.74      5.0  ...     0.4        0   0.2     0.0    2.0   \n",
      "3     6.480  10.80      6.0  ...     0.4        0   0.2     0.0    4.4   \n",
      "4     0.016   0.08      5.0  ...     0.4        0   0.2     0.0    2.0   \n",
      "..      ...    ...      ...  ...     ...      ...   ...     ...    ...   \n",
      "771   0.080   0.89      5.0  ...     0.4        0   0.2     0.0    2.0   \n",
      "772   0.074   0.84      5.0  ...     0.4        0   0.2     0.0    2.0   \n",
      "773   0.008   0.28      5.0  ...     0.4        0   0.2     0.0    2.0   \n",
      "774   0.000   0.20      5.0  ...     0.4        0   0.2     0.0    2.0   \n",
      "775   0.000   0.00      5.0  ...     0.4        0   0.2     0.0    2.0   \n",
      "\n",
      "     Inherent_Risk  CONTROL_RISK  Detection_Risk  Audit_Risk  Risk  \n",
      "0            8.574           0.4             0.5      1.7148     1  \n",
      "1            2.554           0.4             0.5      0.5108     0  \n",
      "2            1.548           0.4             0.5      0.3096     0  \n",
      "3           17.530           0.4             0.5      3.5060     1  \n",
      "4            1.416           0.4             0.5      0.2832     0  \n",
      "..             ...           ...             ...         ...   ...  \n",
      "771          1.578           0.4             0.5      0.3156     0  \n",
      "772          1.568           0.4             0.5      0.3136     0  \n",
      "773          1.456           0.4             0.5      0.2912     0  \n",
      "774          1.440           0.4             0.5      0.2880     0  \n",
      "775          1.464           0.4             0.5      0.2928     0  \n",
      "\n",
      "[774 rows x 27 columns]\n",
      "     PARA_A  Score_A  Risk_A  PARA_B  Score_B  Risk_B  TOTAL  numbers  \\\n",
      "0      4.18      0.6   2.508    2.50      0.2   0.500   6.68      5.0   \n",
      "1      0.00      0.2   0.000    4.83      0.2   0.966   4.83      5.0   \n",
      "2      0.51      0.2   0.102    0.23      0.2   0.046   0.74      5.0   \n",
      "3      0.00      0.2   0.000   10.80      0.6   6.480  10.80      6.0   \n",
      "4      0.00      0.2   0.000    0.08      0.2   0.016   0.08      5.0   \n",
      "..      ...      ...     ...     ...      ...     ...    ...      ...   \n",
      "771    0.49      0.2   0.098    0.40      0.2   0.080   0.89      5.0   \n",
      "772    0.47      0.2   0.094    0.37      0.2   0.074   0.84      5.0   \n",
      "773    0.24      0.2   0.048    0.04      0.2   0.008   0.28      5.0   \n",
      "774    0.20      0.2   0.040    0.00      0.2   0.000   0.20      5.0   \n",
      "775    0.00      0.2   0.000    0.00      0.2   0.000   0.00      5.0   \n",
      "\n",
      "     Score_B.1  Risk_C  ...  District_Loss  PROB  RiSk_E  History  Prob  \\\n",
      "0          0.2     1.0  ...              2   0.2     0.4        0   0.2   \n",
      "1          0.2     1.0  ...              2   0.2     0.4        0   0.2   \n",
      "2          0.2     1.0  ...              2   0.2     0.4        0   0.2   \n",
      "3          0.6     3.6  ...              2   0.2     0.4        0   0.2   \n",
      "4          0.2     1.0  ...              2   0.2     0.4        0   0.2   \n",
      "..         ...     ...  ...            ...   ...     ...      ...   ...   \n",
      "771        0.2     1.0  ...              2   0.2     0.4        0   0.2   \n",
      "772        0.2     1.0  ...              2   0.2     0.4        0   0.2   \n",
      "773        0.2     1.0  ...              2   0.2     0.4        0   0.2   \n",
      "774        0.2     1.0  ...              2   0.2     0.4        0   0.2   \n",
      "775        0.2     1.0  ...              2   0.2     0.4        0   0.2   \n",
      "\n",
      "     Risk_F  Score  Inherent_Risk  CONTROL_RISK  Detection_Risk  \n",
      "0       0.0    2.4          8.574           0.4             0.5  \n",
      "1       0.0    2.0          2.554           0.4             0.5  \n",
      "2       0.0    2.0          1.548           0.4             0.5  \n",
      "3       0.0    4.4         17.530           0.4             0.5  \n",
      "4       0.0    2.0          1.416           0.4             0.5  \n",
      "..      ...    ...            ...           ...             ...  \n",
      "771     0.0    2.0          1.578           0.4             0.5  \n",
      "772     0.0    2.0          1.568           0.4             0.5  \n",
      "773     0.0    2.0          1.456           0.4             0.5  \n",
      "774     0.0    2.0          1.440           0.4             0.5  \n",
      "775     0.0    2.0          1.464           0.4             0.5  \n",
      "\n",
      "[774 rows x 23 columns] 0      1\n",
      "1      0\n",
      "2      0\n",
      "3      1\n",
      "4      0\n",
      "      ..\n",
      "771    0\n",
      "772    0\n",
      "773    0\n",
      "774    0\n",
      "775    0\n",
      "Name: Risk, Length: 774, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "df = pd.read_csv('audit_risk.csv')\n",
    "trial = pd.read_csv('trial.csv')\n",
    "for col in df.columns:\n",
    "    if df[col].isna().any():\n",
    "        print(col)\n",
    "        print(df[df[col].isna()])\n",
    "        print(df[df[col].isna()].index)\n",
    "        print(col)\n",
    "        df = df.loc[~df[col].isna(), :]\n",
    "for col in df.columns:\n",
    "    try:\n",
    "       \n",
    "    \n",
    "        df = df.loc[~(df[col] == 'LOHARU'), :]\n",
    "        df = df.loc[~(df[col] == str), :]\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "\n",
    "# iter_impute = IterativeImputer(max_iter=10, random_state=42)\n",
    "# df = iter_impute.fit_transform(df)\n",
    "\n",
    "print(df)\n",
    "\n",
    "X = df.iloc[:, 2:-2]\n",
    "X = X.apply(pd.to_numeric, errors='coerce')\n",
    "y = df.iloc[:, -1]\n",
    "y = y.apply(pd.to_numeric, errors='coerce')\n",
    "print(X, y)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9935483870967742\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 10):\n",
    "    pre = i*0.1\n",
    "    class_SVC = SVC(kernel='linear', C=pre, random_state=42)\n",
    "    class_SVC.fit(X_train, y_train)\n",
    "\n",
    "    print(class_SVC.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.02774194  0.00258065  0.03290323  0.08774194 -0.00580645  0.0883871\n",
      "  0.09032258  0.04064516  0.04064516  0.04064516  0.10322581  0.11032258\n",
      "  0.11354839  0.00258065  0.00516129  0.01096774  0.01677419  0.01677419\n",
      "  0.01677419 -0.00129032  0.13870968  0.0116129   0.        ]\n",
      "Index(['Inherent_Risk', 'Risk_D', 'Score_MV', 'Money_Value', 'TOTAL', 'Risk_B',\n",
      "       'PARA_B', 'numbers', 'Score_B.1', 'Risk_C', 'Risk_A', 'PARA_A',\n",
      "       'History', 'Prob', 'Risk_F', 'CONTROL_RISK', 'RiSk_E', 'PROB',\n",
      "       'District_Loss', 'Score_A', 'Detection_Risk', 'Score', 'Score_B'],\n",
      "      dtype='object')\n",
      "Gaussian Naive Bayes Accuracy: 0.9612903225806452\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.94      0.97        95\n",
      "           1       0.91      1.00      0.95        60\n",
      "\n",
      "    accuracy                           0.96       155\n",
      "   macro avg       0.95      0.97      0.96       155\n",
      "weighted avg       0.96      0.96      0.96       155\n",
      "\n",
      "Confusion Matrix:\n",
      " [[89  6]\n",
      " [ 0 60]]\n",
      "Accuracy: 0.9612903225806452\n",
      "Precision: 0.964809384164223\n",
      "Recall: 0.9612903225806452\n",
      "F1 Score: 0.9615808455219395\n",
      "ROC AUC Score: 0.968421052631579\n",
      "Average Precision Score: 0.9090909090909091\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.metrics import average_precision_score, precision_recall_curve\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.inspection import permutation_importance\n",
    "gauss_model = GaussianNB()\n",
    "gauss_model.fit(X_train, y_train)\n",
    "imps = permutation_importance(gauss_model, X_test, y_test, n_repeats=10, random_state=42)\n",
    "print(imps.importances_mean)\n",
    "print(X_test.columns[imps.importances_mean.argsort()[::-1]])\n",
    "y_pred = gauss_model.score(X_test, y_test)\n",
    "print(\"Gaussian Naive Bayes Accuracy:\", y_pred)\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, gauss_model.predict(X_test)))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, gauss_model.predict(X_test)))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, gauss_model.predict(X_test)))\n",
    "print(\"Precision:\", precision_score(y_test, gauss_model.predict(X_test), average='weighted'))\n",
    "print(\"Recall:\", recall_score(y_test, gauss_model.predict(X_test), average='weighted'))\n",
    "print(\"F1 Score:\", f1_score(y_test, gauss_model.predict(X_test), average='weighted'))\n",
    "print(\"ROC AUC Score:\", roc_auc_score(y_test, gauss_model.predict(X_test)))\n",
    "print(\"Average Precision Score:\", average_precision_score(y_test, gauss_model.predict(X_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 1.0\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        95\n",
      "           1       1.00      1.00      1.00        60\n",
      "\n",
      "    accuracy                           1.00       155\n",
      "   macro avg       1.00      1.00      1.00       155\n",
      "weighted avg       1.00      1.00      1.00       155\n",
      "\n",
      "Confusion Matrix:\n",
      " [[95  0]\n",
      " [ 0 60]]\n",
      "Accuracy: 1.0\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "F1 Score: 1.0\n",
      "ROC AUC Score: 1.0\n",
      "Average Precision Score: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "y_pred_rf = rf_model.score(X_test, y_test)\n",
    "print(\"Random Forest Accuracy:\", y_pred_rf)\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, rf_model.predict(X_test)))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, rf_model.predict(X_test)))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, rf_model.predict(X_test)))\n",
    "print(\"Precision:\", precision_score(y_test, rf_model.predict(X_test), average='weighted'))\n",
    "print(\"Recall:\", recall_score(y_test, rf_model.predict(X_test), average='weighted'))\n",
    "print(\"F1 Score:\", f1_score(y_test, rf_model.predict(X_test), average='weighted'))\n",
    "print(\"ROC AUC Score:\", roc_auc_score(y_test, rf_model.predict(X_test)))\n",
    "print(\"Average Precision Score:\", average_precision_score(y_test, rf_model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP Classifier Accuracy: 1.0\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        95\n",
      "           1       1.00      1.00      1.00        60\n",
      "\n",
      "    accuracy                           1.00       155\n",
      "   macro avg       1.00      1.00      1.00       155\n",
      "weighted avg       1.00      1.00      1.00       155\n",
      "\n",
      "Confusion Matrix:\n",
      " [[95  0]\n",
      " [ 0 60]]\n",
      "Accuracy: 1.0\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "F1 Score: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp_model = MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=500, random_state=42)\n",
    "mlp_model.fit(X_train, y_train)\n",
    "y_pred_mlp = mlp_model.score(X_test, y_test)\n",
    "print(\"MLP Classifier Accuracy:\", y_pred_mlp)\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, mlp_model.predict(X_test)))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, mlp_model.predict(X_test)))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, mlp_model.predict(X_test)))\n",
    "print(\"Precision:\", precision_score(y_test, mlp_model.predict(X_test), average='weighted'))\n",
    "print(\"Recall:\", recall_score(y_test, mlp_model.predict(X_test), average='weighted'))\n",
    "print(\"F1 Score:\", f1_score(y_test, mlp_model.predict(X_test), average='weighted'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Classifier Accuracy: 0.9870967741935484\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99        95\n",
      "           1       1.00      0.97      0.98        60\n",
      "\n",
      "    accuracy                           0.99       155\n",
      "   macro avg       0.99      0.98      0.99       155\n",
      "weighted avg       0.99      0.99      0.99       155\n",
      "\n",
      "Confusion Matrix:\n",
      " [[95  0]\n",
      " [ 2 58]]\n",
      "Accuracy: 0.9870967741935484\n",
      "Precision: 0.987362820086465\n",
      "Recall: 0.9870967741935484\n",
      "F1 Score: 0.9870546291233825\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn_model = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_model.fit(X_train, y_train)\n",
    "y_pred_knn = knn_model.score(X_test, y_test)\n",
    "print(\"KNN Classifier Accuracy:\", y_pred_knn)\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, knn_model.predict(X_test)))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, knn_model.predict(X_test)))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, knn_model.predict(X_test)))\n",
    "print(\"Precision:\", precision_score(y_test, knn_model.predict(X_test), average='weighted'))\n",
    "print(\"Recall:\", recall_score(y_test, knn_model.predict(X_test), average='weighted'))\n",
    "print(\"F1 Score:\", f1_score(y_test, knn_model.predict(X_test), average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
